{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6eb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, TimestampType, StringType, DoubleType, LongType\n",
    "from pyspark.sql.functions import udf, from_json, col, from_unixtime\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e46a794-04dc-458d-ac26-d3883bdd16c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'CONDA_EXE': '/opt/conda/bin/conda',\n",
       "        '_CE_M': '',\n",
       "        'HOSTNAME': 'f8a3b8050f80',\n",
       "        'PYSPARK_DRIVER_PYTHON': 'python3.9',\n",
       "        'ENV': '',\n",
       "        'PWD': '/app',\n",
       "        'GSETTINGS_SCHEMA_DIR': '/opt/conda/envs/5003-project/share/glib-2.0/schemas',\n",
       "        'CONDA_PREFIX': '/opt/conda/envs/5003-project',\n",
       "        'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '',\n",
       "        'KAFKA_TOPIC_NAME': 'us-election-tweet',\n",
       "        'DB_PORT': '5432',\n",
       "        'KAFKA_CONNECTION_STRING': '',\n",
       "        'DB_USER': 'postgres',\n",
       "        'HOME': '/root',\n",
       "        'LANG': 'C.UTF-8',\n",
       "        'CONDA_PROMPT_MODIFIER': '(5003-project) ',\n",
       "        'PYSPARK_PYTHON': 'python3.9',\n",
       "        'PYTHONPATH': '/app/',\n",
       "        '_CE_CONDA': '',\n",
       "        'DB_HOST': 'db-timescale',\n",
       "        'CONDA_SHLVL': '2',\n",
       "        'SHLVL': '0',\n",
       "        'BASH_ENV': '~/.bashrc',\n",
       "        'DB_POOL_SIZE': '1',\n",
       "        'CONDA_PYTHON_EXE': '/opt/conda/bin/python',\n",
       "        'DB_NAME': '5003-project-dev',\n",
       "        'CONDA_DEFAULT_ENV': '5003-project',\n",
       "        'LC_ALL': 'C.UTF-8',\n",
       "        'PATH': '/opt/conda/envs/5003-project/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
       "        'CONDA_PREFIX_1': '/opt/conda',\n",
       "        'DB_PASS': '5003-project',\n",
       "        '_': '/opt/conda/envs/5003-project/bin/jupyter-lab',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_PARENT_PID': '7',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2b8ef2-fd19-47c0-8bf2-26fb12c0da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_HOST = os.environ.get('DB_HOST', '')\n",
    "DATABASE_PORT = os.environ.get('DB_PORT', '')\n",
    "DATABASE_NAME = os.environ.get('DB_NAME', '')\n",
    "DATABASE_USER = os.environ.get('DB_USER', '')\n",
    "DATABASE_PASS = os.environ.get('DB_PASS', '')\n",
    "ENV = os.environ.get('ENV', 'dev')\n",
    "KAFKA_CONNECTION_STRING = os.environ.get('KAFKA_CONNECTION_STRING', 'kafka-broker:9092')\n",
    "KAFKA_TOPIC_NAME = os.environ.get('KAFKA_TOPIC_NAME', 'us-election-tweet')\n",
    "\n",
    "ENV = 'dev' if ENV == '' else ENV\n",
    "KAFKA_CONNECTION_STRING = 'kafka-broker:9092' if ENV == 'dev' else KAFKA_CONNECTION_STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148d300d-0497-42ef-8b67-9ab79819c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/envs/5003-project/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/envs/5003-project/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-04d39904-c73a-4728-ad78-61c9f6428345;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 405ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-04d39904-c73a-4728-ad78-61c9f6428345\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/14ms)\n",
      "21/11/17 15:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('5003-project')\n",
    "    .master('spark://spark-master:7077')\n",
    "    .config(\"spark.files.overwrite\", \"true\")\n",
    "    .config('spark.driver.extraClassPath', '/opt/drivers/postgresql-42.2.19.jar')\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2')\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8a23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"created_at\", LongType(), True)\n",
    "    .add(\"tweet_id\", DoubleType(), True)\n",
    "    .add(\"tweet\", StringType(), True)\n",
    "    .add(\"likes\", DoubleType(), True)\n",
    "    .add(\"retweet_count\", DoubleType(), True)\n",
    "    .add(\"source\", StringType(), True)\n",
    "    .add(\"user_id\", DoubleType(), True)\n",
    "    .add(\"user_name\", StringType(), True)\n",
    "    .add(\"user_screen_name\", StringType(), True)\n",
    "    .add(\"user_description\", StringType(), True)\n",
    "    .add(\"user_join_date\", TimestampType(), True)\n",
    "    .add(\"user_followers_count\", DoubleType(), True)\n",
    "    .add(\"user_location\", StringType(), True)\n",
    "    .add(\"lat\", DoubleType(), True)\n",
    "    .add(\"long\", DoubleType(), True)\n",
    "    .add(\"city\", StringType(), True)\n",
    "    .add(\"country\", StringType(), True)\n",
    "    .add(\"continent\", StringType(), True)\n",
    "    .add(\"state\", StringType(), True)\n",
    "    .add(\"state_code\", StringType(), True)\n",
    "    .add(\"collected_at\", TimestampType(), True)\n",
    "    .add(\"person\", StringType(), True)\n",
    "    .add(\"time_to_sleep\", DoubleType(), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbb4eed-e98c-491d-a75b-a1715168494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "udf_get_polarity = udf(get_polarity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd68d51d-e6d8-4264-9e84-b8f28cc3aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_CONNECTION_STRING)\n",
    "    .option(\"subscribe\", KAFKA_TOPIC_NAME)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    "    .withColumn('created_at', from_unixtime(col(\"created_at\") / 1000).cast(\"timestamp\"))\n",
    "    .withColumn(\"score\", udf_get_polarity(col(\"tweet\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be88fd87-2254-4a2e-af5b-935faa0e41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/17 15:29:52 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a002e38d-492a-4056-b32b-d8e15bcd5e29. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19057/2047042039.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     df.write.jdbc(url, 'donaldtrump_tweets', mode='append')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m write_biden_table = (df\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/5003-project/lib/python3.9/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/5003-project/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/conda/envs/5003-project/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/5003-project/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/5003-project/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def timescale_sink(df, batch_id):\n",
    "    url = f'jdbc:postgresql://{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}?user={DATABASE_USER}&password={DATABASE_PASS}'\n",
    "    df.filter(df['person']=='biden').write.jdbc(url, 'joebiden_tweets', mode='append')\n",
    "    df.filter(df['person']=='trump').write.jdbc(url, 'donaldtrump_tweets', mode='append')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a246dc3-b452-49fa-8103-96deede63856",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_biden_table = (df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\") \n",
    "    .foreachBatch(timescale_sink)\n",
    "    .start()\n",
    "    .awaitTermination())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11223a8889bed13ee4c511387b17f26535da349a2b521fe727c5339d48cac644"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
